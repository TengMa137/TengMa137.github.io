---
title: 'LLMs Inference speed up EP1 - kv cache'
date: 2024-5-31
permalink: /posts/2024/5/kvcache/
tags:
  - Large Language Model
  - KV cache
  - Attention variants
---


Large Language Models (LLMs) have revolutionized the field of natural language processing, enabling significant advancements in tasks such as language translation, text summarization, and sentiment analysis. However, despite their impressive capabilities, LLMs are not without limitations. One of the most significant challenges facing LLMs today is the problem of inference speed. Due to the sheer size and complexity of these models, the process of making predictions or extracting information from them can be computationally expensive and time-consuming. Several ways to speed up the LLMs without updating hardware: <br>

a. Parallelism: Transformers are designed to work in parallel (Self-attention). One can take advantage of parallel processing on multi-core CPUs or multi-GPU systems.

b. Quantization: Quantizing the weights and activations of neural networks can help reduce the computational requirements for inference, meanwhile save memory usage.

c. Pruning: Pruning techniques can be used to remove redundant connections in neural networks, thereby reducing their size and computational requirements.

d. Knowledge Distillation: This technique involves transferring knowledge from a larger pre-trained model (the teacher) to a smaller model (the student). By leveraging the knowledge learned by the teacher, the student model can achieve better performance with less computational resources.

There are simplier ways, from engineering perspective, to optimizae LLM inference without changing the model architecture: 1. Reduce duplicate calculations (fewer FLOPs), 2. Reduce data transfer time (flash attention)


KV Cache
======

During inference, an LLM produces its output token by token, also known as autoregressive decoding. Each generated token depends on all previous tokens, including those in the prompt and any previously generated output tokens.

Therefore, the Key and Value of previous tokens are involved in the computation for the next token. The long queue of tokens can cause computation bottleneck in the self-attention stage, if we don't use the K, V calculated before. To better illustrate this issue, here's an example shows how self attention works.

The attention score of the first token is obtained by:
$$
Att_1(Q,K,V)=softmax(\frac{Q_1K^T_1}{\sqrt{d}})\vec{V_1}
$$ 
where $K=W_k x$, $Q=W_q x$, $V=W_v x$, $x$ is embedding vector, $d$ is the dimension of embedding vectors, we can overlook it in this discussion. When generating the second token, attention matrix:
$$
\begin{aligned}
Att(Q,K,V) &=softmax(
    \begin{bmatrix}
    Q_1K^T_1       & - \infty \\
    Q_2K^T_1       & Q_2K^T_2 
    \end{bmatrix}
)
\begin{bmatrix}
    \vec{V_1} \\
    \vec{V_2}      
\end{bmatrix} \\
&=
\begin{bmatrix}
    softmax(Q_1K^T_1)\vec{V_1} \\
    softmax(Q_2K^T_1)\vec{V_1} + softmax(Q_2K^T_2)\vec{V_2} 
\end{bmatrix}
\end{aligned}
$$
Attention score for the second token is:
$$
Att_2(Q,K,V)=softmax(Q_2K^T_1)\vec{V_1} + softmax(Q_2K^T_2)\vec{V_2} 
$$
Likewise, we can get the attention score for the thrid token:
$$
Att_3(Q,K,V)=softmax(Q_3K^T_1)\vec{V_1} + softmax(Q_3K^T_2)\vec{V_2} + softmax(Q_3K^T_3)\vec{V_3}
$$
We can see Key and Value used from last iteration are reused to generate the next token, thus store K, V to reduce computation.

![My local chatbot](/images/img/LLM/kvcache.gif)

Nice gif from [joao lages](https://medium.com/@joaolages/kv-caching-explained-276520203249).


<details><summary>Memory is eaten up?</summary>

KV cache can be very large, sometimes up to several GB. Let's see its size if data is stored in fp16 (2 bytes) for a single batch: 
$$
kv_size = 2*2*d*n_layers * max_context_length
$$
Note that for Grouped-query Attention (GQA), multiple heads shared the same Key and Value, which could reduce the kv cache size. 

<br>
<img src='/images/img/LLM/GQA.png' alt="GQA">
<br>

There are research ongoing showing that quantized (with some tricks) KV cache also works as well, things aren't that bad. Overall, more memory consumption for less computation and faster inference, it's a fair trade-off. 

</details>
<br>

PagedAttention
======

In the begining, KV cache is simply saved in VRAM, which requires large contiguous memory allocation. PagedAttention finds that there's a considerable amount of memory being wasted in the KV cache because of excessive reservation. Instead of directly allocate the maximum amount of memory to fill the model context length, no matter how long the real prompt and generation would be, Pagedattention dynamicly allocates memory and save KV cache in non-contiguous blocks:

<br>
<img src='/images/img/LLM/pagedattention.png' alt="pagedattention">
<br>

Pagedattention allows more sophisticated management, for instance, when multiple inference requests share the same large initial system prompt, it's wise to save key and value vectors for the initial prompt once and share among requests. 
Now you can tell why the Pagedattention is so popular: it increases model throughput without any architecture modification, functions as a cache management layer that can be seamlessly integrated with various LLMs, especially useful in scenarios where LLMs handle large batches of prompts. 

Further thoughts on memory management, pre-allocating memory is not bad as long as the allocated blocks are filled. vattension.

Flash Attention
======