---
title: 'Supervised Learning EP1 - Classical ML Models Recap'
date: 2023-07-14
permalink: /posts/2023/07/ML/
tags:
  - Supervised learning
  - Statistical machine learning
---

What is Machine Learning?
======
A concise definition from T. Mitchell: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E". Regarding of what learning is,Herbert A. Simon once defined it as "the ability that a system improves the performance of itself by executing some process". Machine learning is a data driven subject that applies machine learning models to data analyzation and prediction. Generally machine learning can be categorized into: supervised learning, unsupervised learning, reinforcement learning, semi-supervised learning and active learning. To design a machine learning system, basically we need to do the following tasks:<br>
1. Formalize the learning task<br>
2. Collect data<br>
3. Extract features<br>
4. Choose class of learning models<br>
5. Train model<br>
6. Evaluate model<br>

This blog revisits some classical supervised learning Models, including K-nearest neighbours, Decision Tree, Naive Bayes, Perceptron and Support Vector Machine.

K-nearest neighbours
======
<details><summary>CLICK ME</summary>

Here's how a basic KNN model works in classification tasks: Given a new instance $x_new$, find it's K nearest neighbours and then assign $x_new$ to the majority class, aka, majority voting (return mean distances from all K instances in regression tasks).<br> 
We usually pick Euclidean distance to measuring the distance between instances, generally a distance function $d$ should satisfy the following properties: for any instances x, y, z in the sampling set,<br>

1. $d(x, x) = 0;$<br>
2. $d(x, y) = d(y, x);$<br>
3. $d(x, y) + d(y, z) â‰¥ d(x, z);$<br>


Also we'd like to define distance as non-negetive value to avoid troubles, Minkowski distance is the perfect candidate.<br>
Good to know some characteristics of k-nearest neighbour learning:<br>
1. Instance-based learning or lazy learning: The model just memorizes training data. Computation is mostly deferred to the classification phase when there is a test example to be processed. Efficient methods such as kd-tree are usually used to accelarate computation speed.<br>
2. Local learner: assumes prediction should be mainly influenced by nearby instances<br>
3. Uniform feature weighting: all features are uniformly weighted in computing distances
</details>
<br>


Decision Tree
======
<details><summary>CLICK ME</summary>

Usually learning a decision tree contains 3 steps: attribute selection, tree generation and pruning. Classical methods such as ID3 (Quinlan 1986), C4-5 (Quinlan 1993) take a greedy top-down learning strategy. For each ndoe, start from the root with full training set and 
1. Choose the best attribute to be evaluated;
2. Split node training set into children and form child node according to value of chosen attribute;
3. Stop splitting a node if it contains examples from a single class, or there are no more attributes to test.

The best attribute is chosen based on information gain (IG). The IG of attribute A on training dataset D is defined as the difference between the entropy of dataset $H(D)$ and the conditional entropy $H(D|A)$ of D given A. 
$$
IG(D,A) = H(D)-H(D|A)
$$
In information theory, entropy measures the uncertainty of random variables. Given a discrete random variable $X$ that takes a number n of
possible values, we have the probability distribution $P$ and entropy $H$:
$$
P(X=x_i) = p_i, i = 1, 2, ..., n \\
H(X) = - \sum_{i=1}^n p_i log_2 p_i
$$
If we have 2 discrete random variables $(X, Y)$, the conditional entropy $H(Y|X)$ denotes the uncertainty of $Y$ known $X$, as defined below:
$$
H(Y|X) = \sum_{i=1}^n p_i H(Y|X=x_i)
$$

In classification tasks, the entropy of a set of labelled examples $H(D)$ measures its label inhomogeneity.  $H(D|A)$ represents the sum of entropies of subsets of examples obtained partitioning over A values, weighted by their respective sizes. An attribute with high information gain tends to produce homogeneous groups in terms of labels, thus favouring their classification.<br>
The information gain criterion tends to prefer attributes with a large number of possible values. Considering an extreme, the unique ID of each example is an attribute perfectly splitting the data into singletons, but it
will be of no use on new examples. A measure of such spread is the entropy of the dataset wrt the attribute value instead of the class value.
$$
H_A(D) = - \sum_{v \in Values(A)} \frac{\lvert D_v \rvert}{\lvert D \rvert} log_2 \frac{\lvert D_v \rvert}{\lvert D \rvert} 
$$
The information gain ratio (IGR) measures downweights the information gain by such attribute value entropy.
$$
IGR(D, A) = \frac{IG(D, A)}{H_A(D)}
$$
Pruing is necessary, since a complex tree can easily overfit the training set, and sometimes requiring that each leaf has only examples of a certain class can lead to very complex trees. It is possible to accept impure leaves, assigning them the label of the majority of their training examples. **Pre-pruning** decides whether to stop splitting a node even if it contains training examples with different labels, while **post-pruning** learns a full tree and successively prune it removing subtrees Usuallly there is a labeled validate set for **post-pruning** to improve the performance of the model. Here's the procedures:<br>
1. For each node in the tree: evaluate the performance on the validation set when removing the subtree rooted at it;
2. If all node removals worsen performance, STOP;
3. Choose the node whose removal has the best performance improvement;
4. Replace the subtree rooted at it with a leaf;
5. Assign to the leaf the majority label of all examples in the subtree;
6. Return to step 1.

Decision tree also applies to continuous-valued attributes by discreting the continuous values. Discretization threshold can be chosen in order to maximize the attribute quality criterion (e.g. infogain). Procedure:
1. Examples are sorted according to their continuous attribute values;
2. For each pair of successive examples having different labels, a candidate threshold is placed as the average of the two attribute values;
3. For each candidate threshold, the infogain achieved splitting examples according to it is computed;
4. The threshold producing the higher infogain is used to discretize the attribute;

</details>
<br>

Naive Bayes
======
<details><summary>CLICK ME</summary>

Naive Bayes is a classifier based on Bayes' theorem and conditional probability independence assumption. Each input instance $x$ is described by a conjunction of attribute values $(a_1,..., a_m)$. The output class label belongs to s finite label set $Y$. The task is predicting the MAP target value given the instance
$$
\begin{aligned}
y^* = argmax_{y_i \in Y}P(y_i|x) & = argmax_{y_i \in Y} \frac{P(a_1,...,a_m|y_i)P(y_i)}{P(a_1,...,a_m)} \\
& = argmax_{y_i \in Y} P(a_1,...,a_m|y_i)P(y_i)
\end{aligned}
$$
Naive Bayes classifier learns the joint probability distribution of instance and labels, and then predicts the MAP target value for the new instance. However, class conditional probabilities $P(a_1,...,a_m|y_i)$ are hard to learn, as the number of terms is equal to the number of possible instances times the number of target values. Naive Bayes assumption simplifies this problem by assuming that attribute values are independent of each other given the target value:
$$
P(a_1,...,a_m|y_i) = \prod_{j=1}^m P(a_j|y_i) \\
y^* = argmax_{y_i \in Y}\prod_{j=1}^m P(a_j|y_i)P(y_i)
$$
Thus parameters to be learned reduce to the number of possible attribute values times the number of possible target values. The priors $P(y_i)$  can be learned as the fraction of training set instances having each target value, aka maximum-likelihood estimatimation (MLE), while $P(a_j=v_k|y_i=c)$ can also be learned as the fraction of times the attribute value $v_k$ was observed in training examples of class $c$. Suppose there are N instances in the training set, we have:
$$
\begin{aligned}
P(a_j=v_k|y_i=c) &= \frac{\sum_{i=1}^N I(a_j=v_k,y_i=c)}{\sum_{i=1}^N I(y_i=c)}\\
&=\frac{N_{kc}}{N_c}
\end{aligned}
$$
Considering that the probability from MLE could be 0, which would affect the final caculation of Posterior probability and lead to bad results, we use Bayes estimation to add priors of attributes. Assume a Dirichlet prior distribution (with parameters $\alpha_{1c},...,\alpha_{kc}$) for attribute parameters, the posterior distribution for attribute parameters is again multinomial, we have:
$$
P(a_j=v_k|y_i=c) = \frac{N_{kc}+\alpha_{kc}}{N_c+\alpha_{c}}
$$
</details>
<br>

Perceptron
======
<details><summary>CLICK ME</summary>

Perceptron is a Linear classifier for solving binary classification problems. For training dataset $D$, perceptron learns a hyperplane $\omega x + b=0$ that separates instances:
$$
D = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\} \ where\ 
x_i \in R^n, y_i \in \{-1, +1\}\\
f(x)=sign(\omega x + b)
$$
Assume that the dataset is linear separable, i.e. for all instances $x_i$ with positive label $y_i=+1$, $\omega x + b \gt 0$; for all instances $x_i$ with negetive label $y_i=-1$, $\omega x + b \lt 0$. <br>
To find the ideal hyperplane, instead of directly minimizing the total number of misclassified instances, perceptron minimize the sum of distances from misclassified instances $x_i \in M$ to the hyperplane. In this case the loss function is continuously differentiable wrt $(\omega,b)$ and can be optimized by Stochastic Gradient Descent (SGD).
$$
L(\omega,b)=-\sum_{x_i \in M}y_i(\omega x_i+b)
$$
The training procedure:
1. Initialize $\omega_0$, $b_0$;
2. Pick a instance with label $(x_i,y_i)$;
3. If $y_i(\omega x_i+b) \leq 0$, 
$$
\omega = \omega+\eta x_iy_i \\
b = b+\eta y_i
$$
4. Loop over step 2 ~ 3 until there is no misclassified instance.<br>

Note that the ideal hyperplane is not unique, Perceptron could generate different solutions with different initialized $\omega_0$, $b_0$ or non-identical misclassified instances picked during learning.

</details>
<br>


Support Vector Machine
======
<details><summary>Linear SVM</summary>

Support Vector Machine (SVM) is a linear classifier selecting hyperplane maximizing separation margin between classes (large margin classifiers), with solution only depends on a small subset of training examples (support vectors).<br>
Considering classifying a linearly separable dataset $Set_{train}$ into 2 classes:
$$
Set_{train}: \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\} \ where\ 
x_i \in R^n, y_i \in \{-1, +1\}
$$
Usually we will find infinite number of hyperplanes defined by $(\omega, b)$ to seperate the data correctly (e.g. using perceptron), while we can find the optimal hyperplane $(\omega^*, b^*)$ by maximizing the geometric margin $\gamma$:
$$
\gamma = \min \gamma_i,\\
\gamma_i = y_i(\frac{\omega}{\lVert \omega \rVert} x_i+ \frac{b}{\lVert \omega \rVert}), i=1,...,N
$$
Note that for a certain hyperplane $\omega x_i + b = 0$, the distance between instance $x_i$ and the hyperplane is $\frac{1}{\lVert \omega \rVert} \lvert \omega x_i+ b \rvert$. Large distance denotes high confidence, and label class $y_i$ denotes the correctness of classfication for $x_i$.  
That's why SVM is called maximum margin classifier. Finding $(\omega^*, b^*)$ for a hard margin SVM is a constrained optimization problem:
$$
\begin{aligned}
&\max_{\omega,b} \  \gamma\\
&s.t. \  y_i(\frac{\omega}{\lVert \omega \rVert} x_i+ \frac{b}{\lVert \omega \rVert}) \geq\gamma, i=1,...,N
\end{aligned}
$$
Substitute geometric margin with functional margin $\gamma=\frac{\check{\gamma}}{\lVert \omega \rVert}$, where functional margin $\check{\gamma}=y_i(\omega x_i+ b)$:
$$
\begin{aligned}
&\max_{\omega,b} \  \frac{\check{\gamma}}{\lVert \omega \rVert}\\
&s.t. \  y_i(\omega x_i+ b) \geq\check{\gamma}, i=1,...,N
\end{aligned}
$$
Considering that there is an infinite number of equivalent formulation for the same hyperplane:
$$
\begin{aligned}
\omega x_i+ b&=0\\
\alpha(\omega x_i+ b)&=0, \  \forall \alpha \neq0
\end{aligned}
$$
So changing the value of $\check{\gamma}$ will not effect the optimization problem, i.e. the problem is equivalent for any $\check{\gamma}$, so we can substitute $\check{\gamma} = 1$, and convert $\max \frac{1}{\lVert \omega \rVert}$ to its equivalence problem $\min \frac{1}{2} \lVert \omega \rVert ^2$. Now we are dealing with a convex quadratic programming problem (objective is quadratic, points satisfying constraints form a convex set):
$$
\begin{aligned}
&\min_{\omega,b} \  \frac{1}{2} \lVert \omega \rVert ^2\\
&s.t. \  y_i(\omega x_i+ b)-1 \geq0, i=1,...,N
\end{aligned}
$$



</details>
<br>

<details><summary>Non-linear SVM</summary>


</details>
<br>
