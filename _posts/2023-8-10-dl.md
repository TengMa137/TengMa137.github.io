---
title: 'Supervised Learning EP2 - An introduction to Deep Learning'
date: 2023-08-10
permalink: /posts/2023/08/DL/
tags:
  - Supervised learning
  - Deep learning
  - MLP, CNN, RNN, Transformer
---

Deep learning is a subfield of machine learning that involves training artificial neural networks with multiple layers to analyze and solve complex problems. These neural networks are capable of learning hierarchical representations of data, enabling them to extract features and patterns from raw input data. Deep learning has been used in a wide range of applications such as image recognition, natural language processing, and autonomous driving. This blog covers the basics of four different types of DL models, paves the way to further in-depth study of various DNN.

Multi-layer Perceptron
======
<details><summary>CLICK ME</summary>

Multi-Layer Perceptron (MLP) is a type of feedforward artificial neural network (ANN) that consists of multiple layers of interconnected neurons. The output of each neuron in a layer is then passed on to the next layer, and this process continues through all the layers in the architecture. MLPs have shown great success in many artificial intelligence applications, such as image recognition, natural language processing, and speech recognition.<br> Let's review the single layer perceptron first. The perceptron model does a weighted sum operation to an input instance $x$ and uses sign function (activation function) to generate final classification results. MLP expands perceptron by applying different activation functions and sets the output from current perceptron as the input to the next perceptron. Layers between input layer and output layer is called hidden layers, which contain non-linear activation functions.

![SLP vs MLP](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxIM5ebUtSZ06KqB3p1Q3L1s_6pMKr0riYmEoj1-xyuT3kzDBOvxj9K9WJp-koQBvHe8BDtGH-PTjp8Gf9qku1Hj3l4XJbRXPAwRhSL6kHuXgCEy_cL09ri11hCmzRBltxpka1MgJbaARsI8PpijnMwcarTCn68i47xoeMEPKH2ngutLA0XLuYk0erpA/s1640/single%20and%20multi-layer%20perceptron%20image%20combined%202.png)
 
Several commonly used activation functions:
1. Sigmoid
$$
f(x)=\frac{1}{1+exp(-x)}\\
f^{\prime}(x)=\frac{exp(-x)}{(1+exp(-x))^2}=f(x)(1-f(x))
$$
2. Rectified linear unit (Relu)
$$
f(x)= \begin{cases}
x, x \geq 0\\[1ex]
0, x \lt 0
\end{cases}\\
f^{\prime}(x)=\begin{cases}
1, x \geq 0\\[1ex]
0, x \lt 0
\end{cases}
$$
3. Tanh
$$
f(x)=\frac{2}{1+exp(-2x)}-1\\
f^{\prime}(x)=\frac{4 exp(-2x)}{(1+exp(-2x))^2}=1-f(x)^2
$$

![Activation functions](https://pic1.zhimg.com/v2-fcd322da0dc8fee8474147623ba3cb04_r.jpg)

How to set the number of layers and number of neurons per layer? So far we have not found one model structure that fits all problems, we can find a suitable architecture for a specific task using neural architecture search (NAS). Modern deep neural networks usually have tens or hundreds of layers, although we have Universality Approximation Theorem: 2-layer net with linear output with some squashing non-linearity in hidden units can approximate any continuous function over compact domain to arbitrary accuracy (given enough hidden units!)


</details>
<br>

Convolutional Neural Network
======
<details><summary>CLICK ME</summary>

Convolutional neural networks (CNN) are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. CNNs are known for their ability to extract features from images, and they can be trained to recognize patterns within them. You may heard of convolution in signal processing. For 2-D image signal, convolution operation can be written as:
$$
f(x,y)=(g*k)(x,y)=\sum_{m} \sum_{n} g(i-m,i-n) k(m,n)
$$
where $*$ is convolution operation, $g$ is 2-D image data and $k$ is convolution kernel. First rotate the convolution kernel 180 degrees clockwise, and then do element-wise dot production.
In fact, convolution in CNN is not the same convolution operation as mentioned above. It is actually a correlation operation, which directly perform element-wise dot production between kernel and image patches. <br>
In a CNN with multiple layers, neurons in a layer are only connected to a small region of the layer before it, which is so-called local connectivity. One kernel matrix or kernel map is shared between all the patches of an image, namely weight sharing, which allows to learn shift-invariant filter kernels (same feature would be recognized no matter it's location in an image, e.g. a cat will be detected no matter it's on the top left or in the middle of the image) and reduce the number of parameters. <br>Apart from convolutional layers, the design of CNNs involves operations such as pooling layers, non-linearity and fully connected layers. The convolutional layers are responsible for applying filters for feature extraction to the input image, while the pooling layers then reduce the size of the feature map produced by the convolutional layers, and expand the receptive field from local to global. Finally, the fully connected layers are used to  generate prediction result based on the features extracted by the CNN. Take Vgg16 as an example.<br>

![Vgg16](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)

Given a three channel input image (RGB) with size 224\*224, Vgg16 first resizes it to 225\*225 by adding pixels to the input image (padding, usually fill in zero value pixels around the image), and perform convolution using 64 3\*3 conv kernels or filters (obtain 64 feature maps,i.e. output 64 channels, with size 224\*224) and Relu. Padding is to keep the size of feature map same as the input image, since the size will be smaller than 224\*224 after convolution without padding. Perform the same convolution again, then shrink the feature map size to 112\*112 by maxpooling, which picks the maximum of 2\*2 square pixel patch as the output pixel value to halve the side length. Repeat conv+Relu+maxpool, followed by three fully connected layers and softmax (predict the probability of each class). Here's the configuration of vgg convnets.

![Vgg](https://datagen.tech/wp-content/uploads/2022/11/image3-1-1009x1024.png)

Now let's calculate the number of parameters used in a convolution layer and a fully connected layer, to get a better understanding of how a CNN works. A 3\*3 conv kernel has 9 parameters. The first convolution layer uses 64 kernels with size 3\*3 to process a 3-channel image, which has 3\*3\*3\*64 parameters. We can regard it as a cube kernel with size 3\*3\*3, obtained by multiplying kernel size and input channels. **Parameter amount = (conv kernel size * the number of channels of the input feature map) * the number of conv kernels**. The feature map obtained by the last convolution of VGG-16 is 7\*7\*512, which is expanded into a one-dimensional vector 1\*4096 by the first fully connected layer. How to do that? We actually employ 4096 cube kernels with size 7\*7\*512 to do convolution operation on the feature map. The number of parameters of the first FC layer is 7\*7\*512\*4096, a huge number!<br>
Vgg chooses smaller conv kernel compared to the 7\*7 kernel used in Alexnet, which reduce the amount of parameters. Resnet, a famous CNN afer Vgg, introduces a short-cut structure to effectively alleviate the deep network degradation. Batch normalization and regularization operations such as drop out and weight decay are usually used in a CNN, which deserves a new blog to introduce.

</details>
<br>

Recurrent Neural Network
======
<details><summary>CLICK ME</summary>

A Recurrent Neural Network (RNN) is a type of artificial intelligence that is designed to analyze sequence data, such as text or time series data. It consists of a sequence of interconnected nodes, called neurons, which are organized into layers. Each neuron receives input from other neurons in the previous layer and produces an output which is passed on to the next layer. RNNs are typically trained using backpropagation, which involves adjusting the weights of the connections between neurons based on the error between the predicted and actual output. RNNs have shown great success in a wide range of natural language processing tasks, such as speech recognition, machine translation, and text generation. They are also used in many applications related to time series analysis, such as forecasting and trend prediction. 
</details>
<br>

Transformer
======
<details><summary>CLICK ME</summary>

A transformer is a type of neural network architecture that was first introduced in the paper "Attention Is All You Need" by Vaswani et al. It is a deep learning model that uses a self-attention mechanism to allow it to effectively process sequences of input data, such as natural language text or time series data. The transformer architecture uses a stack of layers, each of which contains a set of attention-computing and feedforward neural networks. The attention mechanism allows the model to focus on specific parts of the input sequence and produce a weighted representation of the entire sequence, which can then be used for classification or prediction. <br>
The transformer architecture has shown great success in many natural language processing tasks, including language translation, question answering, and text generation. It is a powerful tool for solving complex problems in artificial intelligence that involve processing sequences of data.


To be continued...
</details>